{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECBM E4040 - Assignment 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to ECBM E4040 Neural Networks & Deep Learning. \n",
    "\n",
    "Deep learning is very popular nowadays both in academia and in industry. In this course, we'll teach you the concepts of neural networks, and how to program your own neural network. \n",
    "\n",
    "The __assignment 0__ is meant to help you get accustomed to the programming environment we use for this course. It consists of 4 parts:\n",
    "* Programming environment setup - Google Compute Engine/local machine, Python, TensorFlow.\n",
    "* How to use Jupyter Notebook\n",
    "* TensorFlow 101\n",
    "* A demo of TensorFlow program\n",
    "\n",
    "<p style='color:red'>The things marked with <strong>'TODO'</strong> requires you to finish. They may also appear in code comments, so please be careful not to miss any.</p>\n",
    "\n",
    "If you have trouble, feel free to contact TAs or post your problem on Piazza.\n",
    "\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Environment setup\n",
    "\n",
    "For the course, we use __Python__ as our programming language, and [__TensorFlow__](https://www.tensorflow.org) as the deep learning framework. Before we start having fun with deep learning, we need to equip ourselves with some knowledge.\n",
    "\n",
    "Our [course website](https://ecbm4040.bitbucket.io) provides a number of tutorials including:\n",
    "1. Python tutorial\n",
    "2. Google Compute Engine setup\n",
    "3. Local environment setup\n",
    "4. Linux tutorial\n",
    "5. Git commands\n",
    "6. TensorFlow tutorial\n",
    "\n",
    "__TODO:__ \n",
    "1. Follow the 2nd tutorial to set up your Google Compute Engine VM instance. __This is required for everyone__. \n",
    "2. Follow the 3rd tutorial to set up your local deep learning environemnt. Since using Google Cloud cost you money, we recommend that you debug your code locally and run it remotely.\n",
    "3. Depending on your understanding of Python, Linux, Git and TensorFlow, the rest tutorials are optional.\n",
    "\n",
    "You may encounter various problems in this part. Don't hesitate to ask for help.\n",
    "\n",
    "After you set up your environment, clone the assignment repo to your VM instance and start working."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - How to use Jupyter Notebook\n",
    "\n",
    "Jupyter Notebook is an interactive Python programming interface. Jupyter Notebook files have a postfix _.ipynb_, and each file is made up of several blocks of code, which we call __cells__. Each cell can be configured as __coding cell__ or __Markdown text cell__. \n",
    "\n",
    "A few basic instructions:\n",
    "\n",
    "* The menu bars are located on the top of a notebook.\n",
    "* To execute a cell, select it, and press `ctrl+Enter`. (You may also try `shift+Enter` and `alt+Enter` to see the difference).\n",
    "* To switch between code and Markdown, select a cell, and select the mode you want in the dropdown menu in the menu bar.\n",
    "\n",
    "A full guide to Jupyter Notebook can be accessed in the _Help_ menu in the menu bar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Jupyter!\n"
     ]
    }
   ],
   "source": [
    "# TODO: to test that you've understood how to use it, make this cell output a string 'Hello Jpuyter!'. \n",
    "# We've written the code, all you need to do is to execute it.\n",
    "print('Hello Jupyter!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - TensorFlow 101\n",
    "\n",
    "TensorFlow is one of the most popular deep learning frameworks now in the world. Originally created by Google, it has received a lot of community support. In this part, we're going to look at some basic TensorFlow concepts and operations, so that you can start playing with it.\n",
    "\n",
    "#### Tow-phase program\n",
    "\n",
    "There are 2 stages of using TensorFlow to create a program. First, we need to __assemle a computation graph__; Then, we use a __session__ to execute operations in that graph. All the operations that you define will not be run until a session is created. This is something like defining a function and use it later. \n",
    "\n",
    "#### The graph\n",
    "The computation graph is a very important concept of TensorFlow. Modern neural networks are usually very complex, and the relations between variables can be hard to catch. In a computation graph, every opearation that you define is recorded as a node. An edge between 2 nodes means data will be exchanged between them. With __TensorBoard__, you can even visualize the graph you've created so far (will be covered later). \n",
    "\n",
    "That's why in TensorFlow, we need to first assemble such a graph, make sure everything is correct, and then execute it. \n",
    "\n",
    "#### High-level APIs\n",
    "There are some other deep learning APIs built upon TensorFlow, providing easy access to users. A typical example is Keras, with which one can write one-line deep learning programs. __Note__: For the course we __prohibit__ the use of other high-level APIs. TensorFlow itself has a good API called TF Learn (tf.contrib.learn). We encourage you to use low-level opeartions and keep away from TF Learn.\n",
    "\n",
    "That's it. Now we proceed to the actual coding!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-aef4a666d9db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Import the TensorFlow module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# The following modules will be used in Part 3 and 4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Maker sure you install the latest version of numpy and matploblib.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# If not, try \"conda install numpy\" and \"conda install matploblib\" in the console (the one that you use to control your VM),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# Import the TensorFlow module\n",
    "import tensorflow as tf\n",
    "# The following modules will be used in Part 3 and 4\n",
    "# Maker sure you install the latest version of numpy and matploblib.\n",
    "# If not, try \"conda install numpy\" and \"conda install matploblib\" in the console (the one that you use to control your VM),\n",
    "# And restart the notebook. \n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello TensorFlow!'\n",
      "b'Hello TensorFlow!'\n"
     ]
    }
   ],
   "source": [
    "# 1. Use of session\n",
    "\n",
    "# By TensorFlow official website, \"A Session instance encapsulates the environment in which Operations\n",
    "#  in a Graph are executed to compute Tensors.\" In short, this is where the computation happens.\n",
    "\n",
    "# Define a string constant\n",
    "string = tf.constant('Hello TensorFlow!')\n",
    "\n",
    "# There are 2 ways of using a session. First one:\n",
    "sess = tf.Session()\n",
    "print(sess.run(string))\n",
    "sess.close()\n",
    "\n",
    "# Second one:\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(string))\n",
    "    \n",
    "# The session doesn't close automatically, so you need to do it manually, otherwise you'll have resource error sometimes.\n",
    "# We recommend the second way because we sometimes forget to put sess.close() at the end of our program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Follow the example above, use TensorFlow to output the string 'YOUR_NAME:YOUR_UNI'. \n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2. Basic math\n",
    "\n",
    "# Define 2 constant nodes. It is a good habit to name your nodes. The name appears in the TensorBoard graph.\n",
    "a = tf.constant(7, dtype=tf.float32, name='a')\n",
    "b = tf.constant(10, dtype=tf.float32, name='b')\n",
    "\n",
    "# Addition and subtraction\n",
    "add = tf.add(a, b, name='add') # same as a+b\n",
    "sub = tf.subtract(a, b, name='sub') # same as a-b\n",
    "\n",
    "# Multiplication and division\n",
    "mul = tf.multiply(a, b, name='mul') # same as a*b\n",
    "div = tf.divide(a, b, name='div') # same as a/b\n",
    "\n",
    "# Power and logarithm\n",
    "power = tf.pow(a, b, name='pow') # same as a^b\n",
    "log = tf.log(a, name='log') # same as log(a)\n",
    "\n",
    "# Launch the session to run these operations\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run([a,b,add,sub,mul,div,power,log])) # Use a list to include all the nodes you want to execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Visit https://www.tensorflow.org/api_guides/python/math_ops, choose any 3 math ops we didn't introduce,\n",
    "# and demonstrate their uses by outputing their results in a session.\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 3. Constant tensor, sequences and random numbers\n",
    "# In TensorFlow, a tensor is an n-dimensional array. 0-d tensor is a scalar. 1-d tensor is a vector, and so on.\n",
    "\n",
    "# We can use TF functions to create all-zero and all-one tensors.\n",
    "zero_array = tf.zeros(shape=[2,3], dtype=tf.float32, name='zero_array')\n",
    "one_array = tf.ones(shape=[2,3], dtype=tf.float32, name='one_array')\n",
    "\n",
    "# Or use a template to infer the shape.\n",
    "template = tf.constant([[1,2,3],[4,5,6]], dtype=tf.float32, name='template') # Has [2,3] shape\n",
    "zero_like = tf.zeros_like(template, name='zero_like')\n",
    "one_like = tf.ones_like(template, name='one_like')\n",
    "\n",
    "# Some sequence generating functions\n",
    "lin_seq = tf.linspace(start=0.0, stop=5.0, num=5, name='lin_seq')\n",
    "lin_range = tf.range(start=0, limit=7, delta=1, name='lin_range')\n",
    "\n",
    "# A random number function\n",
    "norm = tf.random_normal(shape=[5], mean=3, stddev=2.0)\n",
    "\n",
    "# Launch the session to run these operations:\n",
    "with tf.Session() as sess:\n",
    "    print('0 array:', sess.run(zero_array))\n",
    "    print('1 array:', sess.run(one_array))\n",
    "    print('0 inferred:', sess.run(zero_like))\n",
    "    print('1 inferred:', sess.run(one_like))\n",
    "    print('linear sequence:', sess.run(lin_seq))\n",
    "    print('range: ', sess.run(lin_range))\n",
    "    print('Random normal:', sess.run(norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO 1: Generate a 5*5 matrix filled with 9s. \n",
    "# TODO 2: Generate another 5*5 matrix with normal distribution. Choose any mean and stddev you like.\n",
    "# Hint: Visit https://www.tensorflow.org/api_guides/python/constant_op\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 4. Variables\n",
    "# So far, what we've defined are constants, i.e. their values can't be changed. With TensorFlow variables, you can update \n",
    "# values now during training of a network.\n",
    "\n",
    "x = tf.Variable([2,3], dtype=tf.float32) # You need to give an initial value to the variable.\n",
    "\n",
    "# Several ops we can use to change the value of the variable. Note that they all become nodes in the graph.\n",
    "assign = x.assign([4,5])\n",
    "add = x.assign_add([1,1])\n",
    "\n",
    "# To initialize all the variables in the graph, TensorFlow has a global initializer function for us.\n",
    "# You can also use x.initializer to initialize a single variable.\n",
    "# Remember to always intialize a variable before using it. Or you'll run into an error.\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init) # Now all varaibles are assigned their initial values.\n",
    "    print(sess.run(assign)) # print(x.eval()) is the same \n",
    "    print(sess.run(add))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Create a 3*3 tensor variable, then assign some other values to it.\n",
    "# We need to see the initial values and the new values after the assign op to give you full points.\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 5. Placeholders\n",
    "# When you want to feed your data into the network, an intuitive way is to put your data in tf.constant. But that's not clever.\n",
    "# We have placeholders to hold your data. The use is very easy.\n",
    "\n",
    "# Define a placeholder\n",
    "y = tf.placeholder(shape=[5,], dtype=tf.float32) # [5,] or [5] means this is an 1-d array of size 5.\n",
    "z = tf.placeholder(shape=[None, 5], dtype=tf.float32) # Use None in a dimension means any size is acceptable.\n",
    "y_plus = y + 1\n",
    "z_minus = z - 1\n",
    "\n",
    "# Then generate some real arrays to feed into the placeholders\n",
    "feed_y = np.array([1,1,1,1,1], dtype=np.float32) \n",
    "feed_z = np.random.uniform(size=[2,5])\n",
    "\n",
    "# Now use a dictionary to feed the true values into the placeholders.\n",
    "# TensorFlow will detect that the ops we run is linked to some placeholders which need to be fed.\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run([y_plus,z_minus], feed_dict={y: feed_y, z:feed_z}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Create a placeholder of shape 3*3, and a random Variable tensor of shape 3*1. Do a multiplication of them. \n",
    "# You need to create a node for each operation. \n",
    "# Do not define an operation when you actually run it, e.g. sess.run(tf.add(a,b)) is the wrong way. Instead, define c=a+b and \n",
    "# then run it. \n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 6. Indexing\n",
    "# In this part, you will learn some indexing methods like selecting a sequence of data from a tensor. For example, \n",
    "# given data X(100x20) and the corresponding label array y(100x1) , now you want to select out the y[i] \n",
    "# element from the i_th row in X.\n",
    "X = tf.placeholder(shape=[None, 20], dtype=tf.float32)\n",
    "y = tf.placeholder(shape=[None,], dtype=tf.int32)\n",
    "N = tf.shape(X)[0]\n",
    "indices = tf.to_int64(tf.transpose([tf.range(N),y]))\n",
    "out = tf.gather_nd(X, indices)\n",
    "\n",
    "# Then generate some real arrays to feed into the placeholders\n",
    "feed_X = np.random.rand(100, 20).astype('float32')\n",
    "feed_y = np.random.randint(20, size=100)\n",
    "out_np = feed_X[[range(100),feed_y]]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    out_tf = sess.run(out, feed_dict={X:feed_X, y:feed_y})\n",
    "\n",
    "print(\"Is the answer correct (compared with numpy result)? {}\".format(np.allclose(out_np, out_tf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 7. Data type impact\n",
    "# In tensorflow, float type of data includes float32 and float64. Remember that in your later implementation, \n",
    "# you should always consider float32 as your first choice for sake of efficiency, even though it will lose precision.\n",
    "# Here we are going to compare the precision difference between these two types.\n",
    "A32 = tf.Variable([[1,2,3], [4,5,6]], dtype=tf.float32)\n",
    "B32 = A32**2 + 0.1\n",
    "A64 = tf.Variable([[1,2,3], [4,5,6]], dtype=tf.float64)\n",
    "B64 = A64**2 + 0.1\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    B_tf32 = sess.run(B32)\n",
    "    B_tf64 = sess.run(B64)\n",
    "\n",
    "print('float32 result: \\n {}'.format(B_tf32))\n",
    "print('float64 result: \\n {}'.format(B_tf64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've introduced basic TensorFlow operations and concepts. Now, we recommend you to visit the TensorFlow tutorial link provided. It will help you a lot, as the operations we introduced is not sufficient for building a neural network yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 - TensorFlow Demo\n",
    "Part 4 of this assignemnt is a demo, all you need to do is to run it and see the results. It is meant to give you an impression of using TensorFlow.\n",
    "\n",
    "Please run the code and see the outputs. We don't ask you to fully understand the model. However, it is a good practice that you search [www.tensorflow.org](https://www.tensorflow.org) for the functions used in the code. They can be really helpful when you start programming by yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo1: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting ./tmp/data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting ./tmp/data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting ./tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting ./tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch 0 done. Loss: 97022.812510\n",
      "Epoch 1 done. Loss: 38862.111108\n",
      "Epoch 2 done. Loss: 31184.363362\n",
      "Epoch 3 done. Loss: 27861.226439\n",
      "Epoch 4 done. Loss: 25905.874933\n",
      "Epoch 5 done. Loss: 24584.434596\n",
      "Epoch 6 done. Loss: 23599.528761\n",
      "Epoch 7 done. Loss: 22825.517388\n",
      "Epoch 8 done. Loss: 22200.639937\n",
      "Epoch 9 done. Loss: 21686.490717\n",
      "Epoch 10 done. Loss: 21239.721628\n",
      "Epoch 11 done. Loss: 20854.784876\n",
      "Epoch 12 done. Loss: 20520.792423\n",
      "Epoch 13 done. Loss: 20221.754949\n",
      "Epoch 14 done. Loss: 19954.427604\n",
      "Epoch 15 done. Loss: 19705.428426\n",
      "Epoch 16 done. Loss: 19490.808026\n",
      "Epoch 17 done. Loss: 19291.234061\n",
      "Epoch 18 done. Loss: 19105.410657\n",
      "Epoch 19 done. Loss: 18937.064784\n",
      "Epoch 20 done. Loss: 18778.679473\n",
      "Epoch 21 done. Loss: 18629.664913\n",
      "Epoch 22 done. Loss: 18498.043868\n",
      "Epoch 23 done. Loss: 18371.043744\n",
      "Epoch 24 done. Loss: 18246.997320\n",
      "Epoch 25 done. Loss: 18130.592517\n",
      "Epoch 26 done. Loss: 18023.900784\n",
      "Epoch 27 done. Loss: 17920.598697\n",
      "Epoch 28 done. Loss: 17824.614507\n",
      "Epoch 29 done. Loss: 17734.000301\n",
      "Epoch 30 done. Loss: 17640.378907\n",
      "Epoch 31 done. Loss: 17564.784141\n",
      "Epoch 32 done. Loss: 17484.422785\n",
      "Epoch 33 done. Loss: 17404.710477\n",
      "Epoch 34 done. Loss: 17335.751715\n",
      "Epoch 35 done. Loss: 17265.064059\n",
      "Epoch 36 done. Loss: 17196.539677\n",
      "Epoch 37 done. Loss: 17130.054750\n",
      "Epoch 38 done. Loss: 17071.282359\n",
      "Epoch 39 done. Loss: 17016.170201\n",
      "Epoch 40 done. Loss: 16956.220275\n",
      "Epoch 41 done. Loss: 16898.500411\n",
      "Epoch 42 done. Loss: 16847.306268\n",
      "Epoch 43 done. Loss: 16795.323243\n",
      "Epoch 44 done. Loss: 16746.580883\n",
      "Epoch 45 done. Loss: 16695.608347\n",
      "Epoch 46 done. Loss: 16649.419695\n",
      "Epoch 47 done. Loss: 16601.425038\n",
      "Epoch 48 done. Loss: 16559.297766\n",
      "Epoch 49 done. Loss: 16520.634734\n",
      "Accuracy is 91.90001487731934%\n"
     ]
    }
   ],
   "source": [
    "# Demo 1: logistic regression using MNIST dataset\n",
    "\n",
    "# In this demo, we're going to demonstrate a simple multi-class regression model, which is a simple linear classification model.\n",
    "# Reference: https://en.wikipedia.org/wiki/Logistic_regression\n",
    "\n",
    "mnist = input_data.read_data_sets(\"./tmp/data\", one_hot=True) # The data will be stored in ./tmp/data on your machine.\n",
    "num_train = 55000 # Total number of training examples\n",
    "\n",
    "batch_size = 50 # The size of a minibatch\n",
    "epochs = 50 # Train on the entire training set for this much time\n",
    "lr = 0.01 # Learning rate\n",
    "Xte = mnist.test.images # Test data\n",
    "Yte = mnist.test.labels # Test labels\n",
    "\n",
    "# Explicitly set variables in the gpu memory. If you don't have a GPU, comment the 'with tf.device('/gpu\"0')' line,\n",
    "# and remove the following indents.\n",
    "with tf.device('/gpu:0'):\n",
    "    x = tf.placeholder(tf.float32, [None, 784], name='x') # A placeholder to put our input data\n",
    "    y = tf.placeholder(tf.float32, [None, 10], name='y') # A placeholder to put our input labels\n",
    "    W = tf.Variable(tf.random_uniform([784, 10]), dtype=tf.float32, name='weights') # Model weights (trainable)\n",
    "    b = tf.Variable(tf.random_uniform([1, 10]), dtype=tf.float32, name='bias') # Model bias (trainable)\n",
    "    pred = tf.matmul(x, W) + b\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=pred), name='loss') # Model loss function\n",
    "    optimizer = tf.train.GradientDescentOptimizer(lr).minimize(loss) # Use basic gradient descent optimizer\n",
    "    test_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1)) # Test whether the predictions match real labels\n",
    "    accuracy = tf.reduce_mean(tf.cast(test_prediction, tf.float32))\n",
    "\n",
    "# Using minibatch to train the model.\n",
    "num_batch = num_train / batch_size\n",
    "init = tf.global_variables_initializer()\n",
    "# Train the model\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(epochs):\n",
    "        cost_this_epoch = 0\n",
    "        for i in range(int(num_batch)):\n",
    "            xtr, ytr = mnist.train.next_batch(batch_size)\n",
    "            _, l = sess.run([optimizer, loss], feed_dict={x: xtr, y: ytr})\n",
    "            cost_this_epoch += l * batch_size\n",
    "        print('Epoch {} done. Loss: {:5f}'.format(epoch, cost_this_epoch))\n",
    "    accr = sess.run(accuracy, feed_dict={x: Xte, y: Yte})\n",
    "print('Accuracy is {}%'.format(accr * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo2: K-means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-f141955739d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Generate 3 clusters a,b,c, and stack them together as an entire data points.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1231\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultivariate_normal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultivariate_normal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# Demo 2: K-means clustering with multivariate random Gaussian data\n",
    "\n",
    "# In this demo, we're going to demonstrate a K-means clustering algorithm on a set of 2 dimensional data points.\n",
    "# Source code excerpted from https://gist.github.com/dave-andersen/265e68a5e879b5540ebc\n",
    "# Reference: https://en.wikipedia.org/wiki/K-means_clustering\n",
    "\n",
    "# set up inital parameters\n",
    "N = 30000 # Total number of data points\n",
    "K = 3 # Total number of clusters\n",
    "D = 20 # Number of features for each point, here for later visualization, the default D is 2.\n",
    "scale = 20 # Propotional to the distance between each cluster\n",
    "MAX_ITERS = 1000 # max iterations of the algorithm\n",
    "\n",
    "# Generate 3 clusters a,b,c, and stack them together as an entire data points.\n",
    "np.random.seed(1231)\n",
    "a = np.random.multivariate_normal(np.random.randint(-scale, scale, size=D), np.identity(D), size=N//3) \n",
    "b = np.random.multivariate_normal(np.random.randint(-scale, scale, size=D), np.identity(D), size=N//3)\n",
    "c = np.random.multivariate_normal(np.random.randint(-scale, scale, size=D), np.identity(D), size=N//3)\n",
    "d = np.concatenate([a,b,c], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is a basic implementation of K-means clustering\n",
    "def kmeans(data, K=3):\n",
    "    \n",
    "    N, D = data.shape\n",
    "    # clear variables\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # define placeholder for input data as well as Variable for clusters' center and assignment\n",
    "    points = tf.placeholder(tf.float32, [N,D], name='input') # A placeholder to hold our data\n",
    "    cluster_assignments = tf.Variable(tf.zeros([N], dtype=tf.int64)) # A variable to show the cluster assignments for each point\n",
    "    centroids = tf.Variable(tf.random_normal([K,D]))# A variable to show the cluster centers\n",
    "\n",
    "    # distance matrix\n",
    "    # Replicate to N copies of each centroid and K copies of each\n",
    "    # point, then subtract and compute the sum of squared distances.\n",
    "    rep_centroids = tf.reshape(tf.tile(centroids, [N, 1]), [N, K, D])\n",
    "    rep_points = tf.reshape(tf.tile(points, [1, K]), [N, K, D])\n",
    "    sum_squares = tf.reduce_sum(tf.square(rep_points - rep_centroids), \n",
    "                                reduction_indices=2)\n",
    "\n",
    "    # Use argmin to select the lowest-distance point\n",
    "    new_centroids = tf.argmin(sum_squares, 1)\n",
    "    did_assignments_change = tf.reduce_any(tf.not_equal(new_centroids, \n",
    "                                                        cluster_assignments))\n",
    "\n",
    "    # Maximization step: compute the new center of each cluster\n",
    "    def bucket_mean(data, bucket_ids, num_buckets):\n",
    "        total = tf.unsorted_segment_sum(data, bucket_ids, num_buckets)\n",
    "        count = tf.unsorted_segment_sum(tf.ones_like(data), bucket_ids, num_buckets)\n",
    "        count = tf.where(tf.equal(count,tf.zeros_like(count)), tf.ones_like(count), count)\n",
    "        return total / count\n",
    "    means = bucket_mean(points, new_centroids, K)\n",
    "\n",
    "    # If the cluster assignments change, then update the cluster centers. This saves computation.\n",
    "    with tf.control_dependencies([did_assignments_change]):\n",
    "        do_updates = tf.group(\n",
    "            centroids.assign(means),\n",
    "            cluster_assignments.assign(new_centroids))\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Start the session\n",
    "    with tf.Session() as sess:\n",
    "        # initialize all Variables\n",
    "        sess.run(init)\n",
    "        changed = True\n",
    "        iters = 0\n",
    "        while changed and iters < MAX_ITERS:\n",
    "            iters += 1\n",
    "            [changed, _] = sess.run([did_assignments_change, do_updates], feed_dict={points:data})\n",
    "        [centers, assignments] = sess.run([centroids, cluster_assignments], feed_dict={points:data})\n",
    "    # return centers of K clusters and the assignemnt of each point\n",
    "    return iters, centers, assignments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Sometimes, Kmeans does not perform well due to a bad init. When this happens, just try more times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run K-means\n",
    "start = time.time()\n",
    "iters, centers, assignments = kmeans(d, 3)\n",
    "end = time.time()\n",
    "print (\"Found in %.2f seconds\" % (end-start)), iters, \"iterations\"\n",
    "print (\"Centroids:\")\n",
    "print (centers)\n",
    "print (\"Cluster assignments:\", assignments)\n",
    "plt.plot(a[:,0],a[:,1],'.')\n",
    "plt.plot(b[:,0],b[:,1],'x')\n",
    "plt.plot(c[:,0],c[:,1],'+')\n",
    "plt.plot(centers[:,0],centers[:,1],'o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, you will see two different methods of calculating distance matrix in K means. Try to run them and compare difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Another vectorized implementation of distance matrix\n",
    "# Compute the sum of squared distances. Here rather than using tile methods to compute distances \n",
    "# between each point to the centroids, we choose a fully vectorized methods, which can help avoid \n",
    "# the out-of-memory(OOM) problem. Try to re-generate a dataset with shape (30000x2000, N=30000 D=2000) \n",
    "# and K remains to be 3. And run the following code to see the difference between these two methods.\n",
    "\n",
    "points = tf.placeholder(tf.float32, [N,D], name='input') # A placeholder to hold our data\n",
    "centroids = tf.Variable(tf.random_normal([K,D]))# A variable to show the cluster centers\n",
    "\n",
    "# Method 1: Tiling\n",
    "rep_centroids = tf.reshape(tf.tile(centroids, [N, 1]), [N, K, D])\n",
    "rep_points = tf.reshape(tf.tile(points, [1, K]), [N, K, D])\n",
    "sum_squares = tf.reduce_sum(tf.square(rep_points - rep_centroids), \n",
    "                            reduction_indices=2)\n",
    "\n",
    "# Method 2: Vectorized method\n",
    "def distance_matrix(a,b):\n",
    "    '''\n",
    "    inputs:\n",
    "    a - N x D\n",
    "    b - K x D\n",
    "    '''\n",
    "    N = tf.shape(a)[0]\n",
    "    K = tf.shape(b)[0]\n",
    "    a2 = tf.transpose(tf.tile([tf.reduce_sum(a**2, axis=1)], [K,1]))\n",
    "    b2 = tf.tile([tf.reduce_sum(b**2, axis=1)], [N,1])\n",
    "    ab = tf.matmul(a,tf.transpose(b))\n",
    "    \n",
    "    return a2+b2-2*ab\n",
    "\n",
    "sum_squares_vec = distance_matrix(points, centroids)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    try:\n",
    "        tic = time.time()\n",
    "        sess.run(sum_squares, feed_dict={points:d})\n",
    "        toc = time.time()\n",
    "        print(\"computation time of basic distance calculation: {}\".format(toc-tic))\n",
    "    except Exception as e:\n",
    "        print('*'*100)\n",
    "        print(e)\n",
    "        print('*'*100)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    try:\n",
    "        tic = time.time()\n",
    "        sess.run(sum_squares_vec, feed_dict={points:d})\n",
    "        toc = time.time()\n",
    "        print(\"computation time of modified distance calculation: {}\".format(toc-tic))\n",
    "    except Exception as e:\n",
    "        print('*'*100)\n",
    "        print(e)\n",
    "        print('*'*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Application: Clustering & Segmentation\n",
    "\n",
    "Here is an interesting application of K-means clustering algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.misc import imread\n",
    "\n",
    "## Segmetation\n",
    "img = imread('./pics/avengers.jpg')\n",
    "img_points = np.reshape(img,(img.shape[0]*img.shape[1],-1))\n",
    "_, colors, assignments = kmeans(img_points, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seg = np.reshape(colors[assignments,:].astype('uint8'), img.shape)\n",
    "\n",
    "f, axarr = plt.subplots(1,2, figsize=(15,30))\n",
    "axarr[0].imshow(img)\n",
    "axarr[1].imshow(seg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now find more pictures as well as different K and create your own segmentation image.\n",
    "\n",
    "Note: If you encounter out-of-memory(OOM) problem, try to modify the kmeans function by methods mentioned before or resize the input image into a smaller one. Hint: Use numpy.resize "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
