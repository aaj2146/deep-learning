{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECBM4040 Assignment 1, Task 4: Questions\n",
    "\n",
    "1) What is the difference between the SVM method and a neural network, assuming that both work with the same number of training samples N?\n",
    "\n",
    "   Your answer: **The main difference is that SVM converges to a global minimum while neural network can converge to a local minimum. Also, depending upon the number of layers, the time and computational resources required by a neural network may be much larger than those required for an SVM**\n",
    "   \n",
    "\n",
    "\n",
    "2) Why is the ReLU activation function used the most often in neural networks for computer vision?\n",
    "\n",
    "   Your answer: **ReLU is simple, fast and easy to implement. Even the gradient is easy to compute. This becomes especially important in Computer Vision applications where the number of featres i.e pixel vales is large. Also, unlike sigmoid or softmax, ReLU results in a lot of 0s in the matrix which leads to sparsity. With sparser matrices, overfitting can be avoided**\n",
    "   \n",
    "\n",
    "3) Describe your best model in the implementation of the two-layer neural network. Describe your starting point, how you tuned  the hyperparameters, which stategies you did you use to improve the network, show the results of intermediate and final steps.\n",
    "\n",
    "   Your answer: **I updated the weight_scale of 1e-3 instead of the default 1e-1. Also, I numerically stabilised the softmax loss by subtracting the max value of the score matrix from each entry of the score matrix. Also, I used 200 hidden dimensions instead of the default 100**\n",
    "   \n",
    "\n",
    "4) **Cross validation** is a technique used to prove the generalization ability of a model and can help you find a robust set of hyperparameters. Please describe the implementation details of **k-fold cross validation** if you want to use it to find a best set of hyperparameter of the **Linear SVM classification** problem.\n",
    "\n",
    "   Your answer: **In k-fold cross validation, the sample data is divided into k subsets of equal length. In each iteration, a subsample is treated as the validation set while the remaining k-1 samples clubbed together are used as the training set. Now, the cross validation error for each subset is calculated. All k such errors are collected and an average is calculated for a given set of hyerparameters like delta (margin) and gamma (kernel parameter in case data is not linearly seperable). To obtain the optimal hyperparammeters we do the following: For different hyperparameter values an entire iteration of k-fold cross validation is carried out. In the end the set of hyperparameters that results in smallest average error is chosen**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [dlWorksA3]",
   "language": "python",
   "name": "Python [dlWorksA3]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
